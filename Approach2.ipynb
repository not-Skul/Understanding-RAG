{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd8caf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "class VectorIngestionPipeline:\n",
    "    def __init__(self, vector_db, embedding_model):\n",
    "        self.vector_db = vector_db\n",
    "        self.embeddings = embedding_model\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "    \n",
    "    def process_document(self, file_path, metadata=None):\n",
    "        # 1. Extract\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # 2. Chunk & Preprocess\n",
    "        chunks = self.text_splitter.split_documents(documents)\n",
    "        \n",
    "        # 3. Generate embeddings and insert\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            self.ingest_chunk(chunk, i, metadata)\n",
    "    \n",
    "    def ingest_chunk(self, chunk, chunk_id, base_metadata):\n",
    "        # Generate embedding\n",
    "        embedding = self.embeddings.embed_query(chunk.page_content)\n",
    "        \n",
    "        # Enrich metadata\n",
    "        enriched_metadata = {\n",
    "            **base_metadata,\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"content_hash\": hashlib.sha256(chunk.page_content.encode()).hexdigest()\n",
    "        }\n",
    "        \n",
    "        # Insert to vector DB\n",
    "        self.vector_db.upsert(\n",
    "            vectors=[(f\"{base_metadata['source']}_{chunk_id}\", embedding, enriched_metadata)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ce0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "class SmartChunker:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "    def semantic_chunking(self, text, max_chunk_size=1000):\n",
    "        \"\"\"Chunk by sentences while respecting max size\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        sentences = [sent.text.strip() for sent in doc.sents]\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk + sentence) <= max_chunk_size:\n",
    "                current_chunk += sentence + \" \"\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \" \"\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            \n",
    "        return chunks\n",
    "    \n",
    "    def recursive_chunking(self, text):\n",
    "        \"\"\"Use LangChain's recursive splitter for complex documents\"\"\"\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "        return splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2261bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name=\"embeddinggemma:300m\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = OllamaEmbeddings(model=model_name)\n",
    "    \n",
    "    def _get_model_version(self):\n",
    "        return f\"{self.model_name}_v1.0\"\n",
    "    \n",
    "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        try:\n",
    "            return self.model.embed_documents(texts)\n",
    "        except Exception:\n",
    "            return [self.model.embed_query(t) for t in texts]\n",
    "\n",
    "    \n",
    "    def get_embedding_metadata(self):\n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"model_version\": self.model_version,\n",
    "            \"embedding_dimension\": self.model.get_sentence_embedding_dimension(),\n",
    "            \"normalization\": \"l2\"\n",
    "        }\n",
    "# Usage example\n",
    "embedding_manager = EmbeddingManager(\"embeddinggemma:300m\")\n",
    "texts = [\"Sample text 1\", \"Sample text 2\"]\n",
    "embeddings = embedding_manager.embed_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c056b998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langdetect\n",
    "from datetime import datetime\n",
    "import re\n",
    "class MetadataEnricher:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def enrich_metadata(self, text, base_metadata=None):\n",
    "        base_metadata = base_metadata or {}\n",
    "        \n",
    "        enriched = {\n",
    "            **base_metadata,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"word_count\": len(text.split()),\n",
    "            \"char_count\": len(text),\n",
    "            \"language\": self._detect_language(text),\n",
    "            \"content_hash\": hashlib.sha256(text.encode()).hexdigest(),\n",
    "            \"has_numbers\": bool(re.search(r'\\d', text)),\n",
    "            \"has_urls\": bool(re.search(r'http[s]?://', text)),\n",
    "        }\n",
    "        \n",
    "        return enriched\n",
    "    \n",
    "    def _detect_language(self, text):\n",
    "        try:\n",
    "            return langdetect.detect(text)\n",
    "        except:\n",
    "            return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5620e6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "understanding-rag (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
